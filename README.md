<h3><a href="">Vary-toy: The Young's First ``Large'' Vision Language Model</a></h3>
<a href=""><img src="https://img.shields.io/badge/Project-Page-Green"></a>
<a href=""><img src="https://img.shields.io/badge/Paper-PDF-orange"></a> 
<a href=""><img src="https://img.shields.io/badge/demo-blue"></a> 



We present Vary-toy, which is a small-size Vary along with Qwen-1.8B as the base ``Large'' Language Model. Enjoying the same structure and optimization strategy as vanilla Vary, Vary-toy possesses all features of Vary-base (e.g., daily conversation and dense document OCR). Unlike vanilla Vary, we replace negative samples of natural images with positive sample data driven by object detection in the process of generating new vision vocabulary, more fully utilizing the capacity of the vocabulary network and enabling it to efficiently encode visual information corresponding to the natural target perception task. For experiments, Vary-toy can achieve 65.6% ANLS on DocVQA, 59% accuracy on ChartQA, 88.1% accuracy on RefCOCO, and 29% on MMVet.


![image](https://github.com/Ucas-HaoranWei/Vary-toy/assets/50487563/b77603dd-d0eb-4000-b863-2d4a357ee272)

## Release
- [2024/1/11] ðŸ”¥ðŸ”¥ðŸ”¥You only need a single 1080Ti to experience all above features. The code, weights, and online demo will be released in the next week!


